import json
from collections import Counter

import numpy as np
import torch
from imblearn.combine import SMOTEENN
from imblearn.over_sampling import SMOTE


class DataGraph:
    def __init__(self, data_set, feature, label):
        self.data_set = data_set
        assert isinstance(self.data_set, DataSet)
        self.feature = feature
        self.label = label
        pass

    def __len__(self):
        return len(self.data_set), len(self.feature)

    def __getitem__(self, index):
        data_set = self.data_set[index]
        feature = self.feature[index]
        label = self.label
        return data_set, feature, label

    def is_positive(self):
        return self.label == 1


class DataSet:

    def __init__(self, batch_size, hdim):
        self.train_batch = []
        self.valid_batch = []
        self.test_batch = []
        self.batch_size = batch_size
        self.hdim = hdim

        self.positive_indices_in_train = []
        self.negative_indices_in_train = []

        self.train_entries = []
        self.valid_entries = []
        self.test_entries = []

    def add_data_set(self, features, labels, type_data_set):
        data_graph = DataGraph(self, features, labels)
        if type_data_set == 'train':
            self.train_entries.append(data_graph)
        if type_data_set == 'valid':
            self.valid_entries.append(data_graph)
        else:
            self.test_entries.append(data_graph)

    def init_dataset_loader(self):
        entries = []
        train_features = []
        train_targets = []

        for entry in self.train_entries:
            train_features.append(entry.feature)
            train_targets.append(entry.label)

        train_features = np.array(train_features)
        train_targets = np.array(train_targets)
        counter = Counter(train_targets)
        print('Before smote :' + str(counter))
        smote = SMOTE()
        train_features_smt, train_targets_smt = smote.fit_resample(train_features, train_targets)
        counter_smt = Counter(train_targets_smt)
        print('After smote :' + str(counter_smt))

        for feature, label in zip(train_features, train_targets):
            entries.append(DataGraph(self, feature, label))
        self.train_entries = entries

        for tidx, entry in enumerate(self.train_entries):
            if isinstance(entry, DataGraph):
                if entry.label == 1:
                    self.positive_indices_in_train.append(tidx)
                else:
                    self.negative_indices_in_train.append(tidx)
            else:
                pass

        self.initialize_train_batches()
        self.initialize_test_batches()
        self.initialize_valid_batches()

    def create_batch(self, batch_size, entries):
        _batches = []
        if batch_size == -1:
            batch_size = self.batch_size
        total = len(entries)
        indices = np.arange(0, total - 1, 1)
        np.random.shuffle(indices)
        start = 0
        end = len(indices)
        curr = start
        while curr < end:
            c_end = curr + batch_size
            if c_end > end:
                c_end = end
            _batches.append(indices[curr:c_end])
            curr = c_end
        return _batches

    def initialize_train_batches(self):
        self.train_batch = self.create_batch(self.batch_size, self.train_entries)
        return len(self.train_batch)
        pass

    def initialize_valid_batches(self, batch_size=-1):
        if batch_size == -1:
            batch_size = self.batch_size
        self.valid_batch = self.create_batch(batch_size, self.valid_entries)
        return len(self.valid_batch)
        pass

    def get_next_valid_batch(self):
        if len(self.valid_batch_indices) > 0:
            indices = self.valid_batch_indices.pop()
            return self.prepare_data(self.valid_entries, indices)
        raise ValueError('Initialize Valid Batch First by calling dataset.initialize_valid_batches()')

    pass

    def initialize_test_batches(self, batch_size=-1):
        if batch_size == -1:
            batch_size = self.batch_size
        self.test_batch = self.create_batch(batch_size, self.test_entries)
        return len(self.test_batch)
        pass

    def convert_data_to_tensor(self, _entries, indices):
        batch_size = len(indices)
        features = np.zeros(shape=(batch_size, self.hdim), dtype=float)
        targets = np.zeros(shape=(batch_size))
        for tidx, idx in enumerate(indices):
            entry = _entries[idx]
            if isinstance(entry, DataGraph):
                targets[tidx] = entry.label
                for feature_idx in range(self.hdim):
                    features[tidx, feature_idx] = entry.feature[feature_idx]

        return torch.FloatTensor(features), torch.LongTensor(targets)
        pass

    def create_tensor_valid(self):
        if len(self.valid_batch) > 0:
            indices = self.valid_batch.pop()
            return self.convert_data_to_tensor(self.valid_entries, indices)
        raise ValueError('Initialize Valid Batch First by calling dataset.initialize_valid_batches()')
        pass

    def create_tensor_test(self):
        if len(self.test_batch) > 0:
            indices = self.test_batch.pop()
            return self.convert_data_to_tensor(self.test_entries, indices)
        raise ValueError('Initialize Test Batch First by calling dataset.initialize_test_batches()')
        pass

    def create_tensor_train(self):
        if len(self.train_batch) > 0:
            indices = self.train_batch.pop()
            return self.convert_data_to_tensor(self.train_entries, indices)
        raise ValueError('Initialize Train Batch First by calling dataset.initialize_train_batches()')
        pass


def create_data_set_from_json(file_train, file_valid, batch_size=32):
    train_data = json.load(open(file_train))
    valid_data = json.load(open(file_valid))

    dataset = DataSet(batch_size=32, hdim=100)
    print(f'Reading training data from {file_train}')
    for entry in train_data:
        dataset.add_data_set(entry['graph_feature'], int(entry['target']), 'train')
    print(f'Reading training data from {file_valid}')
    for entry in valid_data:
        dataset.add_data_set(entry['graph_feature'], int(entry['target']), 'valid')
    return dataset
