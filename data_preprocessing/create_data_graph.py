import argparse
import csv
import json
import os

import nltk
import numpy as np
from gensim.models import Word2Vec

type_map = {
    'CallExpression': 1, 'InclusiveOrExpression': 2, 'ShiftExpression': 3, 'IdentifierDeclStatement': 4,
    'CompoundStatement': 5, 'IdentifierDecl': 6, 'Condition': 7, 'ArgumentList': 8, 'Sizeof': 9,
    'AdditiveExpression': 10, 'BitAndExpression': 11, 'CFGExitNode': 12, 'SizeofOperand': 13, 'AndExpression': 14,
    'Decl': 15, 'Label': 16, 'Argument': 17, 'Function': 18, 'CastExpression': 19, 'IdentifierDeclType': 20,
    'PtrMemberAccess': 21, 'PostIncDecOperationExpression': 22, 'Identifier': 23, 'GotoStatement': 24,
    'UnaryExpression': 25, 'DeclStmt': 26, 'ClassDefStatement': 27, 'FunctionDef': 28, 'File': 29,
    'ParameterType': 30, 'RelationalExpression': 31, 'SwitchStatement': 32, 'ExpressionStatement': 33,
    'CFGEntryNode': 34, 'MultiplicativeExpression': 35, 'Expression': 36, 'ConditionalExpression': 37,
    'MemberAccess': 38, 'ReturnType': 39, 'UnaryOperator': 40, 'BreakStatement': 41, 'CastTarget': 42,
    'ElseStatement': 43, 'SizeofExpression': 44, 'ClassDef': 45, 'DoStatement': 46, 'Symbol': 47,
    'ExclusiveOrExpression': 48, 'Callee': 49, 'ForStatement': 50, 'InitializerList': 51, 'WhileStatement': 52,
    'Statement': 53, 'ContinueStatement': 54, 'PrimaryExpression': 55, 'ParameterList': 56, 'EqualityExpression': 57,
    'Parameter': 58, 'InfiniteForNode': 59, 'IncDec': 60, 'ArrayIndexing': 61, 'CFGErrorNode': 62, 'IfStatement': 63,
    'ForInit': 64, 'UnaryOperationExpression': 65, 'AssignmentExpression': 66, 'ReturnStatement': 67,
    'OrExpression': 68
}

type_one_hot = np.eye(len(type_map))

edgeType = {
    # 'IS_AST_PARENT': 1,
    #    'IS_CLASS_OF': 2,

    # 'DEF': 4, # Data Flow
    # 'USE': 5, # Data Flow
    # 'REACHES': 6, # Data Flow
    'FLOWS_TO': 3,  # Control Flow
    'CONTROLS': 7,  # Control Dependency edge
    # 'DECLARES': 8,
    # 'DOM': 9,
    # 'POST_DOM': 10,
    # 'IS_FUNCTION_OF_AST': 11,
    # 'IS_FUNCTION_OF_CFG': 12
}


def read_csv(path):
    data = []
    with open(path, 'a+') as file:
        header = file.readline()
        header = header.strip()
        parts = [header.strip() for header in header.split('\t')]
        for line in file:
            line = line.strip()
            instance = {}
            lparts = line.split('\t')
            for i, l in lparts:
                if i < len(lparts):
                    content = lparts[i].strip()
                else:
                    content = ''
                instance[l] = content
            data.append(instance)
        return data


def create_data_graph(node_csv_path, edge_csv_path, target, wv):
    data = dict()
    data["target"] = list()
    data["graph"] = list()
    data["node_features"] = list()

    data["target"].append(target)

    with open(node_csv_path, 'r', encoding='utf-8') as reader:
        nodes = csv.DictReader(reader, delimiter='\t')
        node_map = dict()
        all_nodes = {}
        node_index = 0
        for index, node in enumerate(nodes):

            cfg_node = node['isCFGNode'].strip()
            if cfg_node == '' or cfg_node == 'False':
                continue

            node_key = node['key']
            node_type = node['type']

            if node_type == 'File':
                continue

            node_content = node['code'].strip()
            node_split = nltk.word_tokenize(node_content)
            nrp = np.zeros(100)
            if len(node_split) == 0:
                continue
            for token in node_split:
                try:
                    embedding = wv.wv[token]
                except:
                    embedding = np.zeros(100)
                nrp = np.add(nrp, embedding)
            if len(node_split) > 0:
                fNrp = np.divide(nrp, len(node_split))
            else:
                fNrp = nrp
            node_feature = type_one_hot[type_map[node_type] - 1].tolist()
            node_feature.extend(fNrp.tolist())

            all_nodes[node_key] = node_feature
            node_map[node_key] = node_index
            node_index += 1
        if node_index == 0 or node_index >= 500:
            return None

        all_nodes_with_edges = set()
        trueNodeMap = {}
        all_edges = []

        with open(edge_csv_path, 'r') as ec:
            reader = csv.DictReader(ec, delimiter='\t')
            for e in reader:
                edge = list()
                start, end, eType = e["start"], e["end"], e["type"]
                if eType == "IS_FILE_OF":
                    # We ignore this for now
                    continue
                else:
                    if not start in node_map or not end in node_map:
                        continue
                    all_nodes_with_edges.add(start)
                    all_nodes_with_edges.add(end)
                    if not eType in edgeType:
                        continue
                    edge = [start, edgeType[eType], end]
                    all_edges.append(edge)

        for i, node in enumerate(all_nodes_with_edges):
            trueNodeMap[node] = i
            data["node_features"].append(all_nodes[node])

        for edge in all_edges:
            start, t, end = edge
            start = trueNodeMap[start]
            end = trueNodeMap[end]
            e = [start, t, end]
            data["graph"].append(e)

    return data


def read_file(file_path):
    with open(file_path, "r", encoding="UTF-8") as fp:
        try:
            print('reading file.json in ' + file_path)
            lines = fp.readlines()
        except:
            print("Error reading file.json : %s" % file_path + " at : %s" % lines)

        return ' '.join(lines)


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--csv', help='normalized csv files to process',
                        default='D:\\KHOA LUAN TOT NGHIEP\\folder_data\\chrome_debian\\parsed')
    parser.add_argument('--src', help='source c files to process',
                        default='D:\\KHOA LUAN TOT NGHIEP\\folder_data\\chrome_debian\\raw_code')
    parser.add_argument('--output', help='Output path', default='D:\\KHOA LUAN TOT NGHIEP\\app\\app-repr-vulnerability-alnalysis\\data_graph')
    parser.add_argument('--cfg_only', action='store_true')
    args = parser.parse_args()

    model = Word2Vec.load('D:\\KHOA LUAN TOT NGHIEP\\folder_data\\chrome_debian\\raw_code_deb_chro.100')

    gInputList = list()
    source_files = os.listdir(args.src)
    vul = 0
    non_vul = 0
    i = 0
    done = False
    all_examples = []
    for i, file_path in enumerate(source_files):
        if i % 1000 == 999:
            print(i, vul, non_vul, sep='\t')
        file_path = file_path.strip()
        file_text = read_file(os.path.join(args.src, file_path))
        target = int(file_path[:-2].split('_')[-1])
        nodes_path = os.path.join(args.csv, file_path, 'nodes.csv')
        edges_path = os.path.join(args.csv, file_path, 'edges.csv')
        assert os.path.exists(nodes_path) and os.path.exists(edges_path)
        data = create_data_graph(nodes_path, edges_path, target, model)
        if data is None:
            continue
        if target == 0:
            non_vul += 1
        else:
            vul += 1
        all_examples.append({'code': file_text, 'label': target, 'file_name': file_path})
        gInputList.append(data)

        output_path = args.output + '_full.json'
        text_path = args.output + '_full_text_files.json'

    with open(output_path, 'w') as gi:
        json.dump(gInputList, gi)
        gi.close()

    with open(text_path, 'w') as tp:
        json.dump(all_examples, tp)
        tp.close()
    print(vul, non_vul)
    print('Parsing data graph complete')


if __name__ == '__main__':
    main()
