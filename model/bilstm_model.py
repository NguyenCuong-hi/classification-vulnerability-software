import torch
import torch.nn as nn
import torch.nn.functional as F


class BiLstm(nn.Module):
    import torch


import torch.nn as nn


class BiLstm(nn.Module):
    def __init__(self, input_dim, hidden_dim, dropout_p=0.2):
        super(BiLstm, self).__init__()
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.dropout_p = dropout_p
        self.layer_dropout = nn.Sequential(
            nn.Linear(in_features=self.input_dim, out_features=self.hidden_dim, bias=True),
            nn.ReLU(),
            nn.Dropout(p=self.dropout_p)
        )
        self.layer_bi_lstm = nn.LSTM(self.hidden_dim, hidden_dim, bidirectional=True)
        self.layer_classification = nn.Sequential(
            nn.Linear(in_features=2 * self.hidden_dim, out_features=2),
            nn.LogSoftmax(dim=-1)
        )

    def forward(self, example_batch, targets=None):
        layer_dropout = self.layer_dropout(example_batch)
        lstm_out, _ = self.layer_bi_lstm(layer_dropout.view(len(example_batch), 1, -1))
        lstm_out = lstm_out.view(len(example_batch), -1)
        layer_classification = self.layer_classification(lstm_out)

        props = torch.softmax(layer_classification, dim=1)
        batch_loss = None
        if targets is not None:
            loss_function = nn.CrossEntropyLoss()
            batch_loss = loss_function(layer_classification, targets)

        return props, batch_loss, layer_classification

    def extract_feature(self, x):
        out = self.layer_dropout(x)

        for layer in self.layer_dropout:
            out = layer(out)
        return out